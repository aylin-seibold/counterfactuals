---
title: "Introduction to counterfactuals"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to counterfactuals}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

In this document, we explain the `counterfactuals` workflow for both a classification and a regression task.

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 7,
  fig.height = 3,
  comment = "#>"
)
options(width = 130)
```

```{r setup, message=FALSE}
library(counterfactuals)
library(iml)
library(randomForest)
```


## Classification Models

To illustrate the `counterfactuals` workflow for classification tasks, we search for counterfactual explanations for diabetes tested patients with `MOC`. 

### Data: Pima Indians Diabetes Database

As training data we use the Pima Indians Diabetes Database from the `mlbench` package.
The data set contains 768 observations with 8 features and the binary target variable `diabetes`.

```{r, echo=FALSE}
column_descr = data.frame(
  rbind(
    cbind("pregnant", "Number of times pregnant"),
    cbind("glucose", "Plasma glucose concentration (glucose tolerance test)"),
    cbind("pressure", "Diastolic blood pressure (mm Hg)"),
    cbind("mass", "Body mass index (weight in kg/(height in m)\\^2)"),
    cbind("pedigree", "Diabetes pedigree function"),
    cbind("age", "	Age (years)"),
    cbind("diabetes", "Class variable (test for diabetes)")
  )
)
names(column_descr) <- c("Variable", "Description")
knitr::kable(column_descr, escape = FALSE, format = "html", table.attr = "style='width:100%;'")
```

```{r}
data(PimaIndiansDiabetes, package = "mlbench")  
```


We convert `integerish` features to the `integer` data type to ensure that counterfactuals will
only contain `integer` values for these features (for example no `2.76` pregnancies).

```{r}
PimaIndiansDiabetes$pregnant = as.integer(PimaIndiansDiabetes$pregnant)
PimaIndiansDiabetes$glucose = as.integer(PimaIndiansDiabetes$glucose)
PimaIndiansDiabetes$age = as.integer(PimaIndiansDiabetes$age)
```


### Fitting a model

First, we train a model to predict `diabetes`, leaving out one observation from the training data which is `x_interest`.

```{r}
set.seed(20210816)
rf = randomForest(diabetes ~ ., data = PimaIndiansDiabetes[-499L, ])
```

### Setting up an iml::Predictor() object

An [`iml::Predictor`](https://christophm.github.io/iml/reference/Predictor.html) object serves as a wrapper for different 
model types. It contains the model and the data for its analysis.

```{r}
predictor = Predictor$new(rf, type = "prob")
```


### Find counterfactuals

For `x_interest` the model predicts a class probability of 60.8% for a diabetes disease.

```{r}
x_interest = PimaIndiansDiabetes[499L, ]
predictor$predict(x_interest)
```

Now we examine which risk factors need to be changed to reduce the predicted probability
of diabetes to a maximum of 40%.
Since we want to apply MOC to a classification model, we initialize a `MOCClassif` object.
Individuals whose prediction is further away from the desired prediction than `epsilon` can be penalized. 
Here we set `epsilon = 0` penalizing all individuals with a prediction outside the desired interval. 
With the `fixed_features` argument, we can fix non-actionable features to their current value.

```{r, eval=FALSE}
moc_classif = MOCClassif$new(
  predictor, epsilon = 0, fixed_features = c("pregnant", "age")
)
```

Then, we use the `find_counterfactuals()` method to find counterfactual explanations for `x_interest`. 
As we aim to find counterfactuals with a predicted probability of at most 40% for diabetes, we set the `desired_class` to 
"pos" and the `predicted_prob` to `c(0, 0.4)`. (In the binary classification case, this is equivalent to setting 
`desired_class` to "neg" and `predicted_prob` to `c(0.6, 1)`.)

```{r, eval=FALSE}
cfactuals = moc_classif$find_counterfactuals(
  x_interest, desired_class = "pos", desired_prob = c(0, 0.4)
)
```

```{r, echo=FALSE, results='hide', message=FALSE}
if (!file.exists("introduction-res/cfactuals.RDS")) {
  moc_classif = MOCClassif$new(predictor, epsilon = 0, fixed_features = c("pregnant", "age"))
  cfactuals = moc_classif$find_counterfactuals(x_interest, desired_class = "pos", desired_prob = c(0, 0.4))
  saveRDS(moc_classif, "introduction-res/moc_classif.RDS")
  saveRDS(cfactuals, "introduction-res/cfactuals.RDS")
}
moc_classif = readRDS("introduction-res/moc_classif.RDS")
cfactuals = readRDS("introduction-res/cfactuals.RDS")
```


### The counterfactuals object

The resulting `Counterfactuals` object holds the counterfactuals in the `data` field and possesses several methods for their 
evaluation and visualization.

```{r}
class(cfactuals)
```

Printing a `Counterfactuals` object, gives an overview over the results.
```{r}
print(cfactuals)
```


We can use the `predict()` method to predict the outcome of the counterfactuals.

```{r}
head(cbind(cfactuals$data, cfactuals$predict()), 5L)
```

The `evaluate()` method returns the counterfactuals along with evaluation measures: `dist_x_interest`, `dist_target`,
`nr_changed` and `dist_train`. Setting the `show_diff` argument to `TRUE` displays the counterfactuals as their difference
to `x_interest`: For a numeric feature, positive values indicate an increase compared to the counterfactual feature value 
in `x_interest` and negative values indicate a decrease. For factors, the feature value is displayed if it differs from `x_interest.` 
`NA` means “no difference” in both cases.

```{r}
head(cfactuals$evaluate(show_diff = TRUE), 5L)
```

The `plot_freq_of_feature_changes()` method plots the frequency of feature changes across all counterfactuals. Setting 
`subset_zero = TRUE` removes all unchanged features from the plot.

```{r}
cfactuals$plot_freq_of_feature_changes(subset_zero = TRUE)
```

The parallel plot connects the (scaled) feature values of each counterfactual and highlights `x_interest` in blue.

```{r, message=FALSE, fig.height=3}
cfactuals$plot_parallel()
```

The white dot in the prediction surface plot represents `x_interest`. All counterfactuals that differ from `x_interest` only in the 
selected features are represented as black dots. The tick marks next to the axes indicate the marginal distribution of the
counterfactuals.


```{r, fig.height=3}
cfactuals$plot_surface(feature_names = c("mass", "glucose"))
```


### MOC diagnostics

To evaluate the estimated Pareto front, Dandl et al. (2020) use a hypervolume indicator (Zitzler and Thiele 1998) with a 
reference point that represents the maximal values of the objectives.
The evolution of the hypervolume indicator can be plotted together with the evolution of mean and minimum objective values 
using the `plot_statistics()` method.

```{r, fig.height=2.5, results='asis'}
moc_classif$plot_statistics()
```

Ideally, one would like the mean value of each objective to decrease over the generations.
However, there is often a trade-off between the objectives. Here, the mean values of dist_target and dist_train remain 
relatively constant over the generations, indicating that it is difficult to minimize both objectives concurrently.

This trade-off can also be seen in the scatter plot – created by the `plot_search()` method – that visualizes two selected 
objective values of all individuals. Ideally, one would like to have a point shift to the lower left corner over the generations, 
as this implies better objective values. Here, however, the points are shifted to a middle region, which implies that it is 
difficult to minimize both objectives concurrently.


```{r, fig.height=2.5}
moc_classif$plot_search(objectives = c("dist_train", "dist_target"))
```


## Regression Models

Finding counterfactuals for regression models is analogous to classification models. In this example we search for
counterfactuals explanations for housing prices with `WhatIf`.

### Data: Boston Housing Data

As training data we use the Boston Housing dataset from the `mlbench` package. 
The data set contains 506 observations with 13 features and the (continuous) target variable `medv`.

```{r, echo=FALSE}
# data(BostonHousing, package = "mlbench")  
# df = data.frame(
#   Variable = colnames(BostonHousing),
#   Description = c(
#     "Per capita crime rate by town",
#     "Proportion of residential land zoned for lots over 25,000 sq.ft.",
#     "Proportion of non-retail business acres per town",
#     "Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)",
#     "Nitrogen oxides concentration (parts per 10 million).",
#     "Average number of rooms per dwelling.",
#     "Proportion of owner-occupied units built prior to 1940",
#     "Weighted mean of distances to five Boston employment centres.",
#     "Index of accessibility to radial highways.",
#     "Full-value property-tax rate per $10,000.",
#     "Pupil-teacher ratio by town.",
#     "1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.",
#     "Percentage of lower status of the population",
#     "Median value of owner-occupied homes in USD 1000's"
#   )
# )
# knitr::kable(df, format = "html", table.attr = "style='width:100%;'")
```

```{r}
data(BostonHousing, package = "mlbench")  
```


### Fitting a model

First, we train a model to predict `medv` and again leave out one observation from the training data, which is `x_interest`.

```{r}
set.seed(20210816)
rf = randomForest(medv ~ ., data = BostonHousing[-1L, ])
```

### Setting up an iml::Predictor() object

Then we initialize an [`iml::Predictor`](https://christophm.github.io/iml/reference/Predictor.html) object.

```{r}
predictor = Predictor$new(rf)
```


### Find counterfactuals

For `x_interest` the model predicts a median housing value of 28.32.

```{r}
x_interest = BostonHousing[1L, ]
predictor$predict(x_interest)
```

Since we want to use `WhatIf` to a regression model, we initialize a `WhatIfRegr` object. The argument `n_counterfactuals` 
specifies the number of counterfactuals to be returned.

```{r}
whatif_regr = WhatIfRegr$new(predictor, n_counterfactuals = 5L)
```

Then, we use the `$find_counterfactuals()` method to find counterfactual explanations for `x_interest` with a predicted
housing value in the interval [30, 32].
 
```{r}
cfactuals = whatif_regr$find_counterfactuals(x_interest, desired_outcome = c(30, 32))
```
### The counterfactuals object

As a result, we obtain a `Counterfactuals` object just like for the classification task.

```{r}
cfactuals
```


# References

Dandl, Susanne, Christoph Molnar, Martin Binder, and Bernd Bischl. 2020. “MultiObjective Counterfactual Explanations.” In Parallel Problem Solving from Nature – PPSN XVI, edited by Thomas Bäck, Mike Preuss, André Deutz, Hao Wang, Carola Doerr, Michael Emmerich, and Heike Trautmann, 448–69. Cham: Springer International Publishing.

Zitzler, Eckart, and Lothar Thiele. 1998. “Multiobjective Optimization Using Evolutionary Algorithms—a Comparative Case Study.” In International Conference on Parallel Problem Solving from Nature, 292–301. Springer.
